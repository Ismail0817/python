{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzeB63-h_uTY"
      },
      "source": [
        "# Lab 4 - Model Evaluation\n",
        "\n",
        "Here, we will approach fundamental concepts of model evaluation: \n",
        "\n",
        "1. *Evaluation metrics*\n",
        "2. *K-Fold cross validation*\n",
        "3. *Experimental results* on best performance\n",
        "\n",
        "----\n",
        "\n",
        "```\n",
        "Author:   Luis Quintero\n",
        "Contact:  luis-eduardo@dsv.su.se | luisqtr.com\n",
        "---\n",
        "Notebook based on notes from Christian Kauth (UniFribourg), Jake VanderPlas (DataScienceHandbook), and Zed Lee (DSV,SU)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZnlzAoYFfUW"
      },
      "source": [
        "----\n",
        "\n",
        "First, we import the main Python packages and initialize relevant variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95-KQXJBFfUX"
      },
      "outputs": [],
      "source": [
        "# Numeric analysis\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Data Visualization\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAZbe_vvFfUZ"
      },
      "outputs": [],
      "source": [
        "# Set the seed of the pseudo randomization to guarantee that results are reproducible between executions\n",
        "RANDOM_SEED = 2023\n",
        "np.random.seed(RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CQeaZDHFfUZ"
      },
      "source": [
        "# Loading and performing EDA in real-dataset\n",
        "\n",
        "Let's bring again the *Bank Marketing* dataset available used in Lab 1 and available in the UCI repository: https://archive.ics.uci.edu/ml/datasets/Bank%2BMarketing\n",
        "\n",
        "#### Input variables:\n",
        "\n",
        "**bank client data:**\n",
        "\n",
        "1. `age` (numeric)\n",
        "2. `job` : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n",
        "3. `marital` : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n",
        "4. `education` (categorical: 'primary','secondary','terciary,'unknown')\n",
        "5. `default`: has credit in default? (categorical: 'no','yes','unknown')\n",
        "6. `balance`: average yearly balance, in euros (numeric) \n",
        "7. `housing`: has housing loan? (categorical: 'no','yes','unknown')\n",
        "8. `loan`: has personal loan? (categorical: 'no','yes','unknown')\n",
        "\n",
        "**related with the last contact of the current campaign:**\n",
        "\n",
        "1. `contact`: contact communication type (categorical: 'cellular','telephone')\n",
        "2.  `month`: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n",
        "3.  `day_of_week`: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n",
        "4.  `duration`: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n",
        "\n",
        "**other attributes:**\n",
        "\n",
        "5.  `campaign`: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
        "6.  `pdays`: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n",
        "7.  `previous`: number of contacts performed before this campaign and for this client (numeric)\n",
        "8.  `poutcome`: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n",
        "\n",
        "#### Output variable (desired target):\n",
        "\n",
        "17. `y` - has the client subscribed a term deposit? (binary: 'yes','no')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8apX2vUFfUb"
      },
      "outputs": [],
      "source": [
        "# Pandas provides built-in functions to load text files into DataFrames.\n",
        "data = pd.read_csv(\"bank.csv\", sep=\";\", true_values =[\"yes\"], false_values=[\"no\"])\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXdEv3rHFfUb"
      },
      "outputs": [],
      "source": [
        "data.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ts0Xm82KFfUc"
      },
      "outputs": [],
      "source": [
        "data.describe(include=\"all\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMVAQTQOFfUd"
      },
      "source": [
        "### Organizing data types\n",
        "\n",
        "Above we are describing both numerical and non-numerical features. Therefore, some metrics don't apply for some columns. For example, it's impossible to calculate the `mean` of the feature `education` because it contains only text, not numbers.\n",
        "\n",
        "However, the table is interesting to discover that non-numerical features like `job`, `marital`, `education`, `contact`, `month`, `poutcome` have few **unique** categories. Let's change the data types accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAlAsw7qFfUe"
      },
      "outputs": [],
      "source": [
        "### ORDERED CATEGORICAL FEATURES\n",
        "# The months are incremental, therefore they have an order: 'jan'<'feb'<'mar'<...<'dec'.\n",
        "print(\"List of values in the feature `month`:\", data[\"month\"].unique(), \"and dtype=\",data[\"month\"].dtypes)\n",
        "\n",
        "# We may consider `education` has a variable that has an order. People cannot get reach secondary level, without having primary education.\n",
        "print(\"List of values in the feature `education`:\", data[\"education\"].unique(), \"and dtype=\",data[\"education\"].dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdieBo-oFfUe"
      },
      "outputs": [],
      "source": [
        "# We apply Ordinal Encoding, as in the example from Lab3\n",
        "months_order = [\"jan\",\"feb\",\"mar\",\"apr\",\"may\",\"jun\",\"jul\",\"aug\",\"sep\",\"oct\",\"nov\",\"dec\"]\n",
        "data[\"month\"] = pd.Categorical(data[\"month\"], categories=months_order, ordered=True)\n",
        "\n",
        "print(\"List of values in the feature `month`:\", data[\"month\"].unique(), \"and dtype=\",data[\"month\"].dtypes)\n",
        "\n",
        "# # Another option would be to map the month names to their corresponding numerical value.\n",
        "# # but we will continue with `ordered categories`` for educational purposes\n",
        "# data[\"month\"] = data[\"month\"].map({\n",
        "#                                     \"jan\":1, \"feb\":2, \"mar\":3,\n",
        "#                                     \"apr\":4, \"may\":5, \"jun\":6,\n",
        "#                                     \"jul\":7, \"aug\":8, \"sep\":9,\n",
        "#                                     \"oct\":10,\"nov\":11,\"dic\":12,\n",
        "#                                 })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NK2LJUaTFfUf"
      },
      "outputs": [],
      "source": [
        "# Ordinal encoding for `education`\n",
        "education_order = [\"primary\", \"secondary\", \"tertiary\", \"unknown\"]\n",
        "data[\"education\"] = pd.Categorical(data[\"education\"], categories=education_order, ordered=True)\n",
        "\n",
        "print(\"List of values in the feature `education`:\", data[\"education\"].unique(), \"and dtype=\",data[\"education\"].dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACDyku_mFfUf"
      },
      "outputs": [],
      "source": [
        "# The rest of the categorical variables are unordered.\n",
        "\n",
        "# NOTE: You can also access the column name just using `dot` between the name of the dataframe and the column.\n",
        "# For example, the code to access the feature `data[\"job\"]` can also be written as `data.job`\n",
        "\n",
        "data.job = data.job.astype(\"category\")\n",
        "data.marital = data.marital.astype(\"category\")\n",
        "data.contact = data.contact.astype(\"category\")\n",
        "data.poutcome = data.poutcome.astype(\"category\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lQfcv8AFfUg"
      },
      "outputs": [],
      "source": [
        "data.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQd0wMgNFfUg"
      },
      "source": [
        "### Handling Missing Values and Filtering data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdd_ZnrLFfUg"
      },
      "outputs": [],
      "source": [
        "# Check if there are missing values\n",
        "data.isnull().sum()\n",
        "\n",
        "## There are no missing values (all counts are zero) \n",
        "## If there were, you can delete rows with missing values, or impute values using\n",
        "# data.dropna(inplace=True)\n",
        "## Or impute values\n",
        "# data[\"balance\"].fillna(data[\"balance\"].mean(), inplace=True)\n",
        "# data[\"month\"].fillna(data[\"month\"].value_counts().idxmax(), inplace=True) # For categorical features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTK_w6OrFfUh"
      },
      "source": [
        "For simplicity, let's assume we have some understanding of the business problem and we have selected a subset of features from the whole dataset, which we think may be related to the outcome variable. In real data science projects, it is necessary to experiment with several combinations of features, or even conduct feature engineering to create new variables.\n",
        "\n",
        "Here, we have deliberately chosen some features to have multiple data types and to train classification models: `age`, `marital`, `education`, `default`, `balance`, `loan`, `month`, `duration`, `campaign`, and ***obviously***, the target variable `y`.\n",
        "\n",
        "- The numerical features are: `age`, `balance`, `duration`, `campaign`\n",
        "- The categorical features are: `marital` (unordered), `education` (ordered), `month` (ordered)\n",
        "- The binary features are: `default`, `loan`, and the target `y` (binary classification task)\n",
        "\n",
        "### Conceptual Note❗❗\n",
        "\n",
        "Every decision in the data science pipeline has an influence on the performance of the classification models. *E.g., the data imputation technique, the selection of features, which classifiers to train, which hyperparameters to use in each classifiers, which evaluation metric is needed to compare multiple classifiers.*\n",
        "\n",
        "Therefore, note that here we are making a deliberate decision on the features to choose, which may lead to a very poor performance in the classifiers. The goal of the labs is to learn the tools and how to reason on the results, not to achieve the best classifier ever, so do not worry if in the lab or in your homework the performance is low, it will not determine the grading of the lab. Doing experiments to reach high classification performance is part of **data science research** and maybe of interest if you want to do a thesis in the area."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6Is3DGbFfUh"
      },
      "outputs": [],
      "source": [
        "# Select some of the existing features\n",
        "data_filtered = data[[\"age\",\"marital\",\"education\",\"default\",\"balance\",\"loan\",\"month\",\"duration\",\"campaign\",\"y\"]]\n",
        "data_filtered.describe(include=\"all\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQeGD9EiFfUi"
      },
      "outputs": [],
      "source": [
        "data_filtered[\"marital\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PX6mgICpFfUi"
      },
      "outputs": [],
      "source": [
        "data_filtered[\"education\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-EWO-Y9FfUi"
      },
      "outputs": [],
      "source": [
        "data_filtered.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh43q3prFfUj"
      },
      "source": [
        "Let's also assume that we want to delete the 187 rows with value `unknown` in the `education` feature. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-VrJXwEFfUj"
      },
      "outputs": [],
      "source": [
        "data_filtered = data_filtered[ (data_filtered[\"education\"] != \"unknown\") ] # the operator `!=`` means \"different than\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hGpp2SdFfUj"
      },
      "outputs": [],
      "source": [
        "data_filtered[\"education\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWonSzKpFfUk"
      },
      "outputs": [],
      "source": [
        "data_filtered.shape\n",
        "# The 187 samples were deleted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ed94xDg9FfUk"
      },
      "outputs": [],
      "source": [
        "# Plot class balance\n",
        "data_filtered[\"y\"].value_counts().plot.bar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Skv9p67LFfUk"
      },
      "outputs": [],
      "source": [
        "# Data to be used for the classification task\n",
        "data_filtered.sample(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGk9Rl0GFfUk"
      },
      "source": [
        "Above, we can see that there is much more data from people who did not subscribe to the financial service `y=False`, than people who subscribed `y=True`.\n",
        "\n",
        "Generally, we apply methods to **balance the classes**, but it is outside of the scope of the lab. We just need to keep in mind that certain metrics like `accuracy` are not sufficient to describe the performance of a classifier, and we will need to use other metrics such as `precision`, `recall`, and `f1-score`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5M463yKFfUl"
      },
      "source": [
        "### Preparing dataframes for Classification Tasks\n",
        "\n",
        "We have already loaded the dataset, conducted basic preprocessing and feature selection. The final step is to **transform the pandas DataFrame into numerical Numpy arrays**, so that they can be processed by the packages in `sklearn`. At this stage, we need to **encode the categorical features** as described in Lab3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHd_Gag2FfUl"
      },
      "outputs": [],
      "source": [
        "# Apply Ordinal Encoding in ordered categories to get the numbers\n",
        "labels_month, unique_month = pd.factorize(data_filtered[\"month\"], sort=True)\n",
        "data_filtered[\"month\"] = labels_month\n",
        "\n",
        "labels_edu, unique_edu = pd.factorize(data_filtered[\"education\"], sort=True)\n",
        "data_filtered[\"education\"] = labels_edu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyTIH4fZFfUl"
      },
      "outputs": [],
      "source": [
        "# Apply one-hot encoding on unordered categories to get the columns\n",
        "ohe_marital = pd.get_dummies(data_filtered[\"marital\"])\n",
        "data_filtered = pd.concat([data_filtered, ohe_marital], axis=1)\n",
        "data_filtered = data_filtered.drop([\"marital\"], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOI_TmMPFfUl"
      },
      "outputs": [],
      "source": [
        "# Convert to numeric the columns that are binary\n",
        "data_filtered[\"default\"] = data_filtered[\"default\"].astype(int)\n",
        "data_filtered[\"loan\"] = data_filtered[\"loan\"].astype(int)\n",
        "data_filtered[\"y\"] = data_filtered[\"y\"].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LV9kqWzRFfUm"
      },
      "outputs": [],
      "source": [
        "data_filtered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRxtd3MQFfUm"
      },
      "outputs": [],
      "source": [
        "# Separate the features X and the target variable y\n",
        "data_filtered_X = data_filtered.drop([\"y\"], axis=1)\n",
        "data_filtered_y = data_filtered[\"y\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_MvX7SiFfUm"
      },
      "outputs": [],
      "source": [
        "# Finally, we need transform from Pandas DataFrame to numerical Arrays, and store the column names\n",
        "data_X = data_filtered_X.values\n",
        "data_y = data_filtered_y.values\n",
        "\n",
        "data_colnames = data_filtered_X.columns.values\n",
        "print(data_colnames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbHe1WfrFfUn"
      },
      "source": [
        "### Final feature matrix $\\mathbf{X}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Irq6HXSzFfUn"
      },
      "outputs": [],
      "source": [
        "data_X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TugVoM0tFfUn"
      },
      "source": [
        "### Final target array $\\mathbf{y}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCgipaJJFfUn"
      },
      "outputs": [],
      "source": [
        "data_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uuel9X9IFfUo"
      },
      "source": [
        "# 1. Evaluation metrics\n",
        "\n",
        "Officially we have finished with the preprocessing stage. Now, we have created the two inputs for any classification model ($\\mathbf{X}, y$). The resulting post processed variables `(data_X, data_y)` are equivalent to the synthetic data obtained in previous labs with the functions `make_blobs()` or `make_moons()`. \n",
        "\n",
        "However, we are now using **real datasets** and **more than 2 features**, so we cannot visualize the classification performance by the plots that we had before.\n",
        "\n",
        "The remaining of the lab presents the way to evaluate classification models when we cannot plot the decision boundaries, as in most cases with large datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXfWR1bdFfUo"
      },
      "source": [
        "### Single train-test split\n",
        "\n",
        "From the previous lab, we are using a single train-test split using the command below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qx8ETklIFfUo"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 80/20 train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(data_X, data_y, test_size = 0.2, random_state=RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0l1vhs8FfUo"
      },
      "outputs": [],
      "source": [
        "# An example of the classifier in the data\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "dt_classifier = DecisionTreeClassifier(max_depth=10)\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "y_predicted = dt_classifier.predict(X_test)\n",
        "print(y_predicted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRKtB9AiFfUp"
      },
      "source": [
        "So far, we had visualized the decision boundaries because we only had two variables in the **input feature matrix** $\\mathbf{X}$. Bu we cannot plot the **9 features** in our new dataset.\n",
        "\n",
        "Therefore, we use other proxy metrics to compare how the predicted labels compare with the true labels. **If you check Lab 3 again, note that the variable `y_test` has never been used**, we just inspected the quality of the classification *visually*.\n",
        "\n",
        "The variable `y_test` contains the true labels of the test set and is the main resource to conduct the **numerical evaluation of the classification model**. All numerical scores start from the [**confusion matrix**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSFUx8jxFfUp"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm_results = confusion_matrix(y_test, y_predicted)\n",
        "cm_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsWiAlxOFfUp"
      },
      "outputs": [],
      "source": [
        "# Visual representation of the same Confusion Matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "cm_display = ConfusionMatrixDisplay(cm_results).plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3X0oQZ7FfUp"
      },
      "source": [
        "A lot of metrics for numeric evaluation are inferred from the confusion matrix. It is straightforward with sklearn, and there are many metrics in the [official documentation](https://scikit-learn.org/stable/modules/model_evaluation.html). \n",
        "\n",
        "Note that some metrics are more appropriate depending on whether:\n",
        "- Your dataset is a **binary classification task** or a **multiclass classification task**.\n",
        "- Your dataset is **balanced** or **imbalanced**\n",
        "\n",
        "These are technical details to keep in mind in real data science pipelines, but we will continue with the basic metrics for simplicity. Even though note that the bank dataset is imbalanced and is a binary classification task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JY8gZUA7FfUq"
      },
      "outputs": [],
      "source": [
        "# Individual performance metrics\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "acc_res = accuracy_score(y_test, y_predicted)\n",
        "prec_res = precision_score(y_test, y_predicted)\n",
        "reca_res = recall_score(y_test, y_predicted)\n",
        "f1_res = f1_score(y_test, y_predicted)\n",
        "\n",
        "print(f\"The performance of the DT classifier that we proposed is:\")\n",
        "print(f\"\\t Accuracy=\\t{acc_res}\")\n",
        "print(f\"\\t Precision=\\t{prec_res}\")\n",
        "print(f\"\\t Recall=\\t{reca_res}\")\n",
        "print(f\"\\t F1-score=\\t{f1_res}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mivvF9c4FfUq"
      },
      "outputs": [],
      "source": [
        "# Return a full classification score report\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, y_predicted))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdttlWt9FfUq"
      },
      "source": [
        "\n",
        "### Problem with single train-test partitioning\n",
        "\n",
        "This single train-test split has a significant problem. There is a fraction of the dataset that is holdout for testing and never goes in the training process. \n",
        "\n",
        "#### Reflect ❓\n",
        "- Is this performance acceptable for a binary classification task and an unbalanced dataset?\n",
        "- What would happen if the random split puts into the test set a group of data points that are actually very different from the main data samples (they are outliers)?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjPkN7YxFfUq"
      },
      "source": [
        "# 2. Cross-validation (CV)\n",
        "\n",
        "Cross-validation (CV) is a strategy to solve the problem of single train-test split and **the correct approach** to evaluate models performance, [read more here](https://scikit-learn.org/stable/modules/cross_validation.html), or [here](https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html). The CV partitions a dataset into `k` folds or groups. Then, each fold is part of the test set once. It means that a CV creates `k` different classification models instead of only one, and note that it might be very time consuming for classifiers that take a lot of time training or very large datasets.\n",
        "\n",
        "There are even **better** methods such as [*stratified cross-validation*](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html) that partitions the group following the same proportion in the target class, or the [*leave-one-out cross-validation*](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneOut.html) that is suitable for small datasets. We leave these topics out of the scope of these labs, but feel free to use it in the homeworks or in your own projects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXrr50CjFfUr"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "kf = KFold(n_splits=5)\n",
        "CV_indices = kf.split(data_X, data_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSZDoYtNFfUr"
      },
      "source": [
        "KFold returns a set of groups representing the indices of the train and the test sets. \n",
        "\n",
        "We will use later a simpler approach that calculates the metrics automatically, but **note that** if we iterate over the indices, there are **5 splits** of equal size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfgz1Yp5FfUr"
      },
      "outputs": [],
      "source": [
        "for train_index, test_index in CV_indices:\n",
        "    print(\"TRAIN:\", train_index.shape, \"TEST:\", test_index.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEQcnOhWFfUr"
      },
      "source": [
        "## Model evaluation with automatic CV\n",
        "\n",
        "To apply CV, we start again from the original dataset in `data_X` and `data_y` before they were partitioned.\n",
        "\n",
        "Then, we apply a function that returns an evaluation score after conducting CV [`sklearn.model_selection.cross_validate()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html).\n",
        "\n",
        "Below, we use an example with the same decision tree created a couple of cells above. We perform a 5-fold cross validation, and specify the scoring metrics with a list of string based on the available [scoring parameters](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter). \n",
        "\n",
        "Note that `accuracy` is a *single* number averaged over the confusion matrix, but other metrics like `precision`, `recall` or `f1-score` are computed *per class*. So, in multi-class classification, these metrics need to be averaged over all classes to produce a single number. Sklearn has three types of average: macro-average, micro-average, weighted-average. That is why the scoring metrics below have a suffix. [Read documentation here.](https://scikit-learn.org/stable/modules/model_evaluation.html#from-binary-to-multiclass-and-multilabel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCAX8AaQFfUs"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_validate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSGtaNKNFfUs"
      },
      "outputs": [],
      "source": [
        "# Define the evaluation metrics to be returned by the cross-validation\n",
        "scoring_metrics = [\"accuracy\", \"precision_macro\", \"recall_macro\", \"f1_macro\"] # Metrics of interest\n",
        "\n",
        "# Execute the cross-validation\n",
        "results_eval_dt = cross_validate(dt_classifier, data_X, data_y, cv=5, scoring=scoring_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eh9OUHskFfUs"
      },
      "source": [
        "Each score has a dictionary with the name of the metric and five values corresponding to the 5-folds that we indicated.\n",
        "\n",
        "In addition, the cross validation gives us the training time (`fit_time`) and prediction time (`score_time`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3uvm95SFfUs"
      },
      "outputs": [],
      "source": [
        "results_eval_dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_QXZTuhFfUs"
      },
      "outputs": [],
      "source": [
        "# Averaging the results over the 5-folds and store in a new dictionary\n",
        "averaged_results_dt = {\n",
        "                    \"classifier_name\": \"DT\"\n",
        "                    }\n",
        "\n",
        "# Populate the dictionary with the results of the cross-validation\n",
        "for metric_name, scores in results_eval_dt.items():\n",
        "    averaged_results_dt[metric_name] = [scores.mean()]\n",
        "\n",
        "averaged_results_dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHnMHt4iFfUt"
      },
      "outputs": [],
      "source": [
        "# Create a dataframe with the results\n",
        "results_dt_dataframe = pd.DataFrame(averaged_results_dt)\n",
        "results_dt_dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qomSSu34FfUt"
      },
      "source": [
        "# 3. Experimental evaluation of best performance\n",
        "\n",
        "Now, we have seen how to properly measure the performance of a classifier **numerically**, useful for most classification tasks where the dataset contains many features and the decision boundary cannot be visually inspected.\n",
        "\n",
        "To finalize, we will perform again an experimental evaluation where we follow the same logic than Lab3, but now we also apply **cross validation** and **evaluation metrics** to assess the best performance.\n",
        "\n",
        "In this example, we will just work with two types of classifiers with different hyperparameters. But the experimental approach is scalable for as many classifiers and variants as desired.\n",
        "\n",
        "We will test on the `bank` dataset, the following classifiers:\n",
        "- Random Forest with 10 estimators\n",
        "- Random Forest with 100 estimators\n",
        "- SVM with Linear kernel\n",
        "- SVM with Gaussian kernel\n",
        "\n",
        "The procedure will apply 10-fold cross-validation and the scoring metrics are the same than shown above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCP7qs8NFfUt"
      },
      "outputs": [],
      "source": [
        "# We will apply the classifiers on the normalized dataset, as recommended for KNN and SVM\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "data_X_norm = scaler.fit_transform(data_X)\n",
        "\n",
        "print(f\"data_X min. value: {data_X_norm.min()}, max. value: {data_X_norm.max()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJW9urDPFfUt"
      },
      "outputs": [],
      "source": [
        "# This approach to organize the classifiers is described in Lab 3\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "MODELS_TO_TEST = {\n",
        "    \"RF_10\": RandomForestClassifier(n_estimators=10, max_depth=5),\n",
        "    \"RF_100\": RandomForestClassifier(n_estimators=100, max_depth=5),\n",
        "    \"SVM_lin\": SVC(kernel='linear'),\n",
        "    \"SVM_rbf\": SVC(kernel='rbf'),\n",
        "}\n",
        "\n",
        "# Define the number of splits \n",
        "NUMBER_OF_SPLITS = 10\n",
        "\n",
        "# Scoring metrics\n",
        "SCORING_METRICS = [\"accuracy\", \"precision_macro\", \"recall_macro\", \"f1_macro\"] # Metrics of interest\n",
        "\n",
        "# Create empty DataFrame to populate  the name of the classifier and the six values returned from `cross_validate()`\n",
        "results_evaluation = pd.DataFrame({\n",
        "                                    \"classifier_name\":[],\n",
        "                                    \"fit_time\": [],\n",
        "                                    \"score_time\": [],\n",
        "                                    \"test_accuracy\": [],\n",
        "                                    \"test_precision_macro\": [],\n",
        "                                    \"test_recall_macro\": [],\n",
        "                                    \"test_f1_macro\": [],\n",
        "                                    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6L326GFQFfUu"
      },
      "outputs": [],
      "source": [
        "#### ITERATION FOR THE EXPERIMENT\n",
        "\n",
        "for name, classifier in MODELS_TO_TEST.items():\n",
        "    \n",
        "    print(f\"Currently training the classifier {name}.\")\n",
        "\n",
        "    # Get the evaluation metrics per fold after cross-validation\n",
        "    # Note that we are passing the normalized array `data_X_norm` to all classifiers\n",
        "    scores_cv = cross_validate(classifier, data_X_norm, data_y, cv=NUMBER_OF_SPLITS, scoring=SCORING_METRICS)\n",
        "\n",
        "    # Average the scores among folds\n",
        "    dict_this_result = {\n",
        "                    \"classifier_name\":[name],\n",
        "                    }\n",
        "    # Populate the dictionary with the results of the cross-validation\n",
        "    for metric_name, score_per_fold in scores_cv.items():\n",
        "        dict_this_result[metric_name] = [ score_per_fold.mean() ]\n",
        "\n",
        "    #### Generate the results to populate the pandas.DataFrame\n",
        "    this_result = pd.DataFrame(dict_this_result)\n",
        "\n",
        "    # Append to the main dataframe with the results \n",
        "    results_evaluation = pd.concat([results_evaluation, this_result], ignore_index=True)\n",
        "\n",
        "print(\"The experimental setup has finished\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDtlS0jNFfUu"
      },
      "outputs": [],
      "source": [
        "results_evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLFWrxweFfUv"
      },
      "source": [
        "### ❓ - Optional tasks and reflection: \n",
        "\n",
        "The labs on classification are officially over. We have finished evaluating multiple classifiers for a single dataset and getting the numerical evaluation metrics.\n",
        "\n",
        "Below, there are some **optional** extra tasks if you want to continue exploring, but **they are not the HW**:\n",
        "\n",
        "- Try to visualize the final results using plots as we did in the lab 3.\n",
        "- Which was the best classifier on average? and the worst?\n",
        "- Which was the fastest to train? and the fastest to predict? Why?\n",
        "- *Advanced:* Instead of testing RF with predefined `n_estimators=10` or `100`, try to setup a similar experiment finding the optimal hyperparameter among a set of options (*Hint: Grid Search*)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmjebAi64a8a"
      },
      "source": [
        "----\n",
        "\n",
        "# Homework - Lab 4\n",
        "\n",
        "The homework consists on submitting the Jupyter notebook started in Lab3, but extended with a numerical evaluation of the classification models on a public dataset of your choice.\n",
        "\n",
        "**INSTRUCTIONS:**\n",
        "\n",
        "Continue working on the same jupyter notebook from the Lab 3 where you worked on the following sections:\n",
        " 1. Choose a dataset for your case study\n",
        " 2. Create a blank jupyter notebook\n",
        " 3. Basic EDA\n",
        " 4. Classification\n",
        "\n",
        "## 5. Evaluation\n",
        "\n",
        "Add more cells below to develop the tasks below. You will propose and run an experimental evaluation on your dataset:\n",
        "\n",
        "1. Use the public dataset from Lab3.\n",
        "2. Select a subset of features (minimum 3) at your own choice. Remember also to include the **target variable** to run a classification task.\n",
        "3. Apply ordinal encoding or one-hot encoding to the categorical variables, if you have chosen any.\n",
        "4. Design an experiment where you apply the three classification models (DT, RF, KNN) from Lab3 over your dataset.\n",
        "5. The experiment should output, for each classifier, the average accuracy, and macro average of precision, recall, and f1-score. Also apply the mean of the results per fold after cross-validation, as shown in the Lab 4.\n",
        "6. Make at least two plots where you summarize the experimental results graphically. \n",
        "7. *Analyze in a markdown cell:* Which classification model seems to perform better in your data? Would you deploy it in a real-life task? Why or why not?\n",
        "8. *Final reflection, in a markdown cell:* So far you chose a handful of classifiers with predefined hyperparameters. Describe briefly how do you think you can determine experimentally which hyperparameter performs better for a given classifier? **Note that you do not need to implement this evaluation,** just describe in a couple of sentences the steps or Python functions that you would use for this purpose.\n",
        "\n",
        "9. **Deliverable:** Send your jupyter notebook `DSHI_Lab3and4_yourname_yourlastname.ipynb` together with the dataset file(s) necessary to execute the notebook. Follow this checklist before submitting your homework:\n",
        "  * [x] When you think that your Jupyter notebook is finished\n",
        "  * [ ] Check that your notebook contains the code to generate the plots from Lab3.\n",
        "  * [ ] Clean the variables in memory by `restarting the kernel/runtime`\n",
        "  * [ ] Restart the execution of the notebook from the beginning clicking on the option `Run All`\n",
        "  * [ ] Verify that the notebook runs until the last cell without errors in the middle.\n",
        "  * [ ] If there are errors, you need to fix them until it runs properly from beginning to end.\n",
        "  * [ ] Check again that a clean run of your notebook finishes without errors.\n",
        "  * [ ] Save the notebook (or download it if you are working on Google Colab).\n",
        "  * [ ] Compress the notebook `.ipynb` and the ***original*** dataset file(s) in a `.zip` file.\n",
        "  * [ ] Submit the `.zip` file on iLearn\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epMvP1wF9JGL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}